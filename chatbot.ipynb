{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DloXKRisXes9",
        "colab_type": "code",
        "outputId": "4d955f70-4f5a-46d7-83ed-a548fd55b2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=ca0226ff64ee285d6801156941441df993473d1f32707a707fddf7dc7faf04dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ZhDWABi4Dy",
        "colab_type": "code",
        "outputId": "58e521f2-11de-4444-bfa3-e5762dcbad4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCROdGSV8eq",
        "colab_type": "code",
        "outputId": "3b752201-8f27-42a7-94e9-226f68be4012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT1XM4x6jakt",
        "colab_type": "code",
        "outputId": "38fde5e3-482b-42de-84ec-34da3e181a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7etJwOPXnuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=open('/content/HR.txt.txt','r',errors='ignore')\n",
        "raw=data.read()\n",
        "data1=open('/content/CV.txt','r',errors='ignore')\n",
        "raw1=data1.read()\n",
        "raw=raw+raw1\n",
        "raw=raw.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acFjMXKRYjJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens=nltk.sent_tokenize(raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L--RrIamZGhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(text):\n",
        "  remove_punct_dict= dict((ord(punct),None) for punct in string.punctuation)\n",
        "  ##### WORD TOKENIZATION\n",
        "  word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "  ##### REMOVE ASCII\n",
        "  new_words = []\n",
        "  for word in word_token:\n",
        "    new_word = unicodedata.normalize('NFKD',word).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "    new_words.append(new_word)\n",
        "\n",
        "  ##### REMOVE TAGS\n",
        "  rmv=[]\n",
        "  for w in new_words:\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "    rmv.append(text)\n",
        "\n",
        "  ##### POS TAGGING AND LEMMATIZATION\n",
        "  tag_map = defaultdict(lambda : wn.NOUN)\n",
        "  tag_map['J'] = wn.ADJ\n",
        "  tag_map['V'] = wn.VERB\n",
        "  tag_map['R'] = wn.ADV\n",
        "  lmtzr = WordNetLemmatizer()\n",
        "  lemma_list = []\n",
        "  rmv = [i for i in rmv if i]\n",
        "  for token, tag in nltk.pos_tag(rmv):\n",
        "    lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "    lemma_list.append(lemma)\n",
        "  return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNpY_AeZZQtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "welcome_input = (\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",)\n",
        "welcome_response = [\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\"]\n",
        "def welcome(user_response):\n",
        "  for word in user_response.split():\n",
        "    if word.lower() in welcome_input:\n",
        "      return random.choice(welcome_response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCuJ_2kUadBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "#wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)' or 'what is (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjLzh2AJeKut",
        "colab_type": "code",
        "outputId": "49117987-4951-4efa-d965-3b1fedac2965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "flag=True\n",
        "print(\"\"\"HELLO! My Name is Jarvis and I'm a Chatbot. If you want to exit, type \"Bye!\" \"\"\")\n",
        "while (flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response not in ['bye','shutdown','exit','quit']):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"JARVIS: You are welcome.\")\n",
        "    else:\n",
        "        if(welcome(user_response)!=None):\n",
        "          print(\"JARVIS: \"+welcome(user_response))\n",
        "        else:\n",
        "            print(\"JARVIS: \",end=\"\")\n",
        "            print(generateResponse(user_response))\n",
        "            sent_tokens.remove(user_response)\n",
        "\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"JARVIS: Bye!!!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HELLO! My Name is Jarvis and I'm a Chatbot. If you want to exit, type \"Bye!\" \n",
            "hey\n",
            "JARVIS: hi\n",
            "tell me about coronavirus\n",
            "JARVIS: Checking Wikipedia\n",
            "Coronaviruses are a group of related RNA viruses that cause diseases in mammals and birds. In humans, these viruses cause respiratory tract infections that can range from mild to lethal. Mild illnesses include some cases of the common cold (which is caused also by certain other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS, and COVID-19.\n",
            "what is maternity leave\n",
            "JARVIS: maternity leave\n",
            "according to the section 5 of the maternity benefit act 1961, it is woman employee right to claim for maternity leave for 26 weeks for 2. such woman employee can apply maternity leave just eight weeks before the date of are expected delivery.\n",
            "what are the primary responsibilities of human resource manager\n",
            "JARVIS: primary responsibilities of the human resource manager:\n",
            "to develop a thorough knowledge of corporate culture, plans and policies.\n",
            "tell me about india\n",
            "JARVIS: Checking Wikipedia\n",
            "India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya), is a country in South Asia. It is the second-most populous country, the seventh-largest country by area, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\n",
            "shutdown\n",
            "JARVIS: Bye!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA18NsoLf7yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}