{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DloXKRisXes9",
        "colab_type": "code",
        "outputId": "7bbe892e-79e7-488d-948f-97cfc7469e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=30c21d27cc47c6c66fbe8755eca219f530e4e5cb7bc1efc3c7e2e2d8029b155c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ZhDWABi4Dy",
        "colab_type": "code",
        "outputId": "a2a66e71-0683-42c2-fc8f-009ba5171ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCROdGSV8eq",
        "colab_type": "code",
        "outputId": "a06cbb45-2b8d-40e5-9da5-79d0d401e8d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT1XM4x6jakt",
        "colab_type": "code",
        "outputId": "2690f512-3003-45d2-fca8-94f16abeaa2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7etJwOPXnuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=open('/content/HR.txt.txt','r',errors='ignore')\n",
        "raw=data.read()\n",
        "data1=open('/content/CV.txt','r',errors='ignore')\n",
        "data2=open('/content/movie_lines.txt','r',errors='ignore')\n",
        "raw1=data1.read()\n",
        "raw2=data2.read()\n",
        "raw=raw.lower()\n",
        "raw1=raw1.lower()\n",
        "raw2=raw2.lower()\n",
        "raw=raw+raw1+raw2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acFjMXKRYjJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens=nltk.sent_tokenize(raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L--RrIamZGhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(text):\n",
        "  remove_punct_dict= dict((ord(punct),None) for punct in string.punctuation)\n",
        "  ##### WORD TOKENIZATION\n",
        "  word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "  ##### REMOVE ASCII\n",
        "  new_words = []\n",
        "  for word in word_token:\n",
        "    new_word = unicodedata.normalize('NFKD',word).encode('ascii','ignore').decode('utf-8','ignore')\n",
        "    new_words.append(new_word)\n",
        "\n",
        "  ##### REMOVE TAGS\n",
        "  rmv=[]\n",
        "  for w in new_words:\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "    rmv.append(text)\n",
        "\n",
        "  ##### POS TAGGING AND LEMMATIZATION\n",
        "  tag_map = defaultdict(lambda : wn.NOUN)\n",
        "  tag_map['J'] = wn.ADJ\n",
        "  tag_map['V'] = wn.VERB\n",
        "  tag_map['R'] = wn.ADV\n",
        "  lmtzr = WordNetLemmatizer()\n",
        "  lemma_list = []\n",
        "  rmv = [i for i in rmv if i]\n",
        "  for token, tag in nltk.pos_tag(rmv):\n",
        "    lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "    lemma_list.append(lemma)\n",
        "  return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNpY_AeZZQtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "welcome_input = (\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",)\n",
        "welcome_response = [\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\"]\n",
        "def welcome(user_response):\n",
        "  for word in user_response.split():\n",
        "    if word.lower() in welcome_input:\n",
        "      return random.choice(welcome_response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCuJ_2kUadBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "#wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)' or 'what is (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjLzh2AJeKut",
        "colab_type": "code",
        "outputId": "5c15ebf7-e8cb-48b2-c3e4-4cf26a38a06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "flag=True\n",
        "print(\"\"\"HELLO! My Name is Jarvis and I'm a Chatbot. If you want to exit, type \"Bye!\" \"\"\")\n",
        "while (flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response not in ['bye','shutdown','exit','quit']):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"JARVIS: You are welcome.\")\n",
        "    else:\n",
        "        if(welcome(user_response)!=None):\n",
        "          print(\"JARVIS: \"+welcome(user_response))\n",
        "        else:\n",
        "            print(\"JARVIS: \",end=\"\")\n",
        "            print(generateResponse(user_response))\n",
        "            sent_tokens.remove(user_response)\n",
        "\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"JARVIS: Bye!!!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HELLO! My Name is Jarvis and I'm a Chatbot. If you want to exit, type \"Bye!\" \n",
            "hi\n",
            "JARVIS: I am glad! You are talking to me\n",
            "whats up\n",
            "JARVIS: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA18NsoLf7yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}